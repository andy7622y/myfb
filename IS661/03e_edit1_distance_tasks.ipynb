{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72a69b52",
      "metadata": {
        "id": "72a69b52"
      },
      "source": [
        "# Exercise 03: Minimum Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6ffbf5d0",
      "metadata": {
        "id": "6ffbf5d0"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Dict, Set, Tuple\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06714771",
      "metadata": {
        "id": "06714771"
      },
      "source": [
        "### Task 1: Preprocessing\n",
        "In this task, use functions from ``nltk`` wherever possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "76219894",
      "metadata": {
        "id": "76219894"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3d5b19",
      "metadata": {
        "id": "7a3d5b19"
      },
      "source": [
        "__a)__ Write a function that removes the punctuation (consider ``string.punctuation``) and all stopwords from a given text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6566c136",
      "metadata": {
        "id": "6566c136",
        "outputId": "2c9f4201-7944-45be-d3d9-4b0fbe3e55d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.util import StreamBackedCorpusView"
      ],
      "metadata": {
        "id": "-oQfbeZacsXO"
      },
      "id": "-oQfbeZacsXO",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def clean(tokens: Union[List[str], StreamBackedCorpusView]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of tokens by removing punctuation and stopwords.\n",
        "\n",
        "    :param tokens: list of tokens that should be cleaned\n",
        "    :return: list of cleaned tokens\n",
        "    \"\"\"\n",
        "    # Get the stopwords in English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # If the input is a StreamBackedCorpusView, convert it to a list\n",
        "    if isinstance(tokens, StreamBackedCorpusView):\n",
        "        tokens = list(tokens)\n",
        "\n",
        "    # Clean the tokens by removing stopwords and punctuation\n",
        "    cleaned_tokens = [\n",
        "        token for token in tokens\n",
        "        if token.lower() not in stopwords\n",
        "    ]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"This is an example sentence, with punctuation and stopwords!\"\n",
        "tokens = text.split()  # Tokenize the example text\n",
        "cleaned_tokens = clean(tokens)\n",
        "print(cleaned_tokens)\n"
      ],
      "metadata": {
        "id": "o5Tr2NhFcutz",
        "outputId": "d1bbe502-e4c9-4a05-ce72-eb1a2d5770b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "id": "o5Tr2NhFcutz",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argument of type 'WordListCorpusReader' is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bb9d40d1fa27>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is an example sentence, with punctuation and stopwords!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tokenize the example text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcleaned_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-bb9d40d1fa27>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Clean the tokens by removing stopwords and punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     cleaned_tokens = [\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-bb9d40d1fa27>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     cleaned_tokens = [\n\u001b[1;32m     15\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3a5cae",
      "metadata": {
        "id": "cb3a5cae"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
        "\n",
        "def clean(tokens: Union[List[str], StreamBackedCorpusView]) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param tokens: list of tokens that should be cleaned\n",
        "    :return: list of cleaned tokens\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "176ba8fb",
      "metadata": {
        "id": "176ba8fb",
        "outputId": "62a0be56-8b7c-4485-9a86-91f11cf0611d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<WordListCorpusReader in '/root/nltk_data/corpora/stopwords'>\n"
          ]
        }
      ],
      "source": [
        "res = i for i in tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a263ee46",
      "metadata": {
        "id": "a263ee46"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "t1 = \"I am happy to have you my dear friend!~\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f8ac9d6",
      "metadata": {
        "id": "3f8ac9d6",
        "outputId": "1232dbda-faa6-45bc-fd33-6c2e610f7f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['happy', 'dear', 'friend!~']\n"
          ]
        }
      ],
      "source": [
        "print(clean(t1.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2a8270",
      "metadata": {
        "id": "bf2a8270"
      },
      "source": [
        "__b)__ Get the list of tokens from the book Moby Dick (``melville-moby_dick.txt``) which you can find in the ``gutenberg`` corpus. Return the word counts sorted by frequency (most frequent word first) before and after cleaning the list of tokens with your function from __a)__.\n",
        "\n",
        "_Hint: Take a look at_ ``defaultdict`` _and_ ``Counter`` _from the Python module_ ``collections``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1bfe52",
      "metadata": {
        "id": "de1bfe52"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a1e0b1",
      "metadata": {
        "id": "77a1e0b1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d3fdfcef",
      "metadata": {
        "id": "d3fdfcef"
      },
      "source": [
        "### Task 2: Minimum Edit Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d1f213",
      "metadata": {
        "id": "90d1f213"
      },
      "source": [
        "__a)__ In this task we want to calculate the minimum edit distance between two input strings.\n",
        "\n",
        "Write a function that implements the minimum edit distance algorithm (dynamic programming) from the lecture. Your function should get two strings as input and should return the minimum edit distance and a 2-dimensional numpy array that represents the corresponding edit distance table ``D``. Assume that insertion and deletion have a cost of 1. The cost of substitution should be specified by the user (default: 2, i.e., Levenshtein distance).\n",
        "\n",
        "Note, that the entry ``D[i][j]`` in the returned numpy array should correspond to the cell D(i,j) of the edit distance table from the lecture (i.e. the numpy array is a vertically flipped version of the table from the lecture).\n",
        "\n",
        "__Example:__ _The edit distance table_ ``D`` _for a source string of length 3 and a target string of length 5 should be initialized as follows:_\n",
        "\n",
        "```\n",
        "[[0  1  2  3  4  5]\n",
        " [1  0  0  0  0  0]\n",
        " [2  0  0  0  0  0]\n",
        " [3  0  0  0  0  0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df3ce3bc",
      "metadata": {
        "id": "df3ce3bc"
      },
      "outputs": [],
      "source": [
        "def edit_distance(str1: str, str2: str, cost_of_substitute: int=2) -> (int, NDArray[NDArray[int]]):\n",
        "    '''\n",
        "    :param str1: source string\n",
        "    :param str2: target string\n",
        "    :cost_of_substitute: cost for substituting mismatching letters\n",
        "    :returns:\n",
        "        - minimum edit distance of str1 and str2\n",
        "        - minimum edit distance table\n",
        "    '''\n",
        "    # your code here\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668db831",
      "metadata": {
        "id": "668db831"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a57a8a",
      "metadata": {
        "id": "e6a57a8a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}