{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "080be2e1",
      "metadata": {
        "id": "080be2e1"
      },
      "source": [
        "# Home Assignment 3 (30pts)\n",
        "\n",
        "Submit your solution via Ilias until 23.59h on Friday, November 15th. Late submissions are **not possible**.\n",
        "\n",
        "Submit your solutions in teams of 4 students. Unless explicitly agreed otherwise in advance, submissions from teams with more or less members will NOT be graded (i.e., count as failed).\n",
        "\n",
        "**Make sure that all team members are part of the submitting group on Ilias.**\n",
        "\n",
        "You may use the code from the exercises and basic functionalities that are explained in the official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
        "\n",
        "#### General guidelines:\n",
        "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
        "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions.\n",
        "* Ensure that the notebook does not rely on the current notebook or system state!\n",
        "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that\n",
        "    are not in scope anymore.\n",
        "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
        "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
        "* Ensure your code/notebook terminates in reasonable time.\n",
        "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
        "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
        "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
        "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ed9164",
      "metadata": {
        "id": "02ed9164"
      },
      "source": [
        "Additional packages (if any):\n",
        " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "36ef36f1",
      "metadata": {
        "id": "36ef36f1"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Dict, Set, Tuple\n",
        "from numpy.typing import NDArray\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4acc6b92",
      "metadata": {
        "id": "4acc6b92"
      },
      "source": [
        "### Task 1: POS tagging (6 points)\n",
        "\n",
        "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTKâ€™s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "01d59e75",
      "metadata": {
        "id": "01d59e75",
        "outputId": "ad8410a2-d024-4f3d-b8fc-19c004c49992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# NLTK's off-the-shelf POS tagger\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e78bb0d",
      "metadata": {
        "id": "8e78bb0d"
      },
      "source": [
        "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. __(2 pts)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5651149b",
      "metadata": {
        "id": "5651149b"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
        "\n",
        "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
        "    '''\n",
        "    :param corpus: sequence of tokens that represents the text corpus\n",
        "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
        "    '''\n",
        "    res = {}\n",
        "    pos = nltk.pos_tag(corpus) # get the tags of the corpus\n",
        "    for w, tag in pos:\n",
        "      if tag not in res:\n",
        "        res[tag] = set() # if the tag is new, then build the empty set\n",
        "      res[tag].add(w) # add the word into tag\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba737321",
      "metadata": {
        "id": "ba737321"
      },
      "source": [
        "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag.\n",
        "\n",
        "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context.\n",
        "\n",
        "You can assume that the training corpus is large enough to include all possible POS tags. __(2 pts)__\n",
        "\n",
        "_Hint: consider the_ ``random`` _module_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "LUWbBnb8jzRP"
      },
      "id": "LUWbBnb8jzRP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0b4efad8",
      "metadata": {
        "id": "0b4efad8"
      },
      "outputs": [],
      "source": [
        "def generate_rand(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool=False) -> List[List[str]]:\n",
        "    '''\n",
        "    :param sentence: input sentence that sets the tag pattern\n",
        "    :param pos_dict: maps each tag to a list of associated words\n",
        "    :param n: number of sentences that should be generated\n",
        "    :return: List of sentences with the same tag structure as the input sentence\n",
        "    '''\n",
        "    pos_sen = nltk.pos_tag(sentence)\n",
        "    sequences = [i for _,i in pos_sen] # get the sequences of tag\n",
        "    res = []\n",
        "    while len(res) < n:\n",
        "      senlist = [] # prepare for one sentence\n",
        "\n",
        "      for tag in sequences:\n",
        "        word = random.choice(list(pos_dict[tag])) # for each tag, randomly choose one word\n",
        "        senlist.append(word) # add the word into sentence\n",
        "\n",
        "      if better_quality:\n",
        "        new_sequences = [i for _,i in nltk.pos_tag(senlist)] # check if the new tag sequences keeps the same\n",
        "        if new_sequences == sequences:\n",
        "          res.append(senlist) # only consider the right one\n",
        "\n",
        "      else:\n",
        "        res.append(senlist) # generate the sentences\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a9b3ba",
      "metadata": {
        "id": "54a9b3ba"
      },
      "source": [
        "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
        "\n",
        "* \"Emma\" by Jane Austen\n",
        "\n",
        "* The \"King James Bible\"\n",
        "\n",
        "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. __(2 pts)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a69ab118",
      "metadata": {
        "id": "a69ab118",
        "outputId": "9b791aa1-0f48-49a0-8e05-aca90d62bc25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('test', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('very', 'RB'),\n",
              " ('difficult', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]\n",
        "nltk.pos_tag(sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "id": "lu6qAYz2ndKL",
        "outputId": "4e587a72-78a7-437b-d75a-309dad44d68b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lu6qAYz2ndKL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emma_text = gutenberg.words('austen-emma.txt')\n",
        "bible_text = gutenberg.words('bible-kjv.txt')"
      ],
      "metadata": {
        "id": "Oou4jtPko_Ij"
      },
      "id": "Oou4jtPko_Ij",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ad042eec",
      "metadata": {
        "id": "ad042eec",
        "outputId": "a6b3c6c6-cb57-46a3-e781-0d9b7adb86c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'instruction', 'mounts', 'cordially', 'want'], ['some', 'memory', 'conjectures', 'really', 'female'], ['each', 'turn', 'delights', 'naturally', 'invaluable'], ['these', '_each_', 'don', 'playfully', 'fashionable'], ['all', '?\"--', '.\\'\"', 'No', 'ungrateful'], ['no', 'custom', 'turns', 'resolutely', 'capricious'], ['both', 'stop', 'wins', 'pointedly', 'dulness'], ['This', 'awoke', 'draws', 'impertinently', 'appetite'], ['envy', 'tranquil', 'amuses', 'intently', 'past'], ['doth', 'commission', 'inclines', 'misunderstood', 'Such']]\n",
            "[['neither', 'threescore', 'retaineth', 'courteously', 'forcible'], ['any', 'tradition', 'kindleth', 'unfaithfully', 'endless'], ['No', 'tiling', 'offences', 'earthly', 'worthy'], ['either', 'weakness', 'garlands', 'expressly', 'covetous'], ['This', 'bathe', 'longeth', 'rather', 'durable'], ['Some', 'displeasure', 'Hittites', 'quietly', 'ravenous'], ['any', 'leg', 'Levites', 'soberly', 'Ephesian'], ['every', 'paintedst', 'fruits', 'noon', 'angel'], ['The', 'eastward', 'snuffeth', 'zealously', 'ignorant'], ['Every', 'greediness', 'leadeth', 'outward', 'delectable']]\n"
          ]
        }
      ],
      "source": [
        "emma_tags = collect_words_for_tag(emma_text)\n",
        "bible_tags = collect_words_for_tag(bible_text)\n",
        "\n",
        "emma_sent = generate_rand(sent, emma_tags, 10)\n",
        "bible_sent = generate_rand(sent, bible_tags, 10, better_quality=True)\n",
        "print(emma_sent)\n",
        "print(bible_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beecad4e",
      "metadata": {
        "id": "beecad4e"
      },
      "source": [
        "### Task 2: The Viterbi algorithm (12 points)\n",
        "Implement the Viterbi algorithm as introduced in the lecture and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
        "\n",
        "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found.\n",
        "\n",
        "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
        "\n",
        "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8319309",
      "metadata": {
        "id": "e8319309"
      },
      "outputs": [],
      "source": [
        "# test sentence\n",
        "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
        "\n",
        "# state transition probabilities (complete)\n",
        "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
        "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
        "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
        "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
        "\n",
        "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
        "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
        "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9e4d09",
      "metadata": {
        "id": "aa9e4d09"
      },
      "outputs": [],
      "source": [
        "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> (List[str], float):\n",
        "    '''\n",
        "    :param sentence: sentence that we want to tag\n",
        "    :param trans_prob: dict with state transition probabilities\n",
        "    :param emiss_prob: dict with word emission probabilities\n",
        "    :param beam: beam size for beam search. If 0, don't apply beam search\n",
        "    :returns:\n",
        "        - list with POS tags for each input word\n",
        "        - float that indicates the probability of the tag sequence\n",
        "    '''\n",
        "    for n in len(sentence):\n",
        "      word = sentence[i]\n",
        "      for _, tag in emiss_prob:\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3ab66f",
      "metadata": {
        "id": "7a3ab66f"
      },
      "source": [
        "### Task 3: ML Basics - Naive Bayes Classification (12pts)\n",
        "In this task, we want to build a Naive Bayes classifier with add-1 smoothing as introduced in the lecture for text classification (pseudocode given below), e.g., to assign a category to a document. Use the class-skeleton provided below for your implementation.\n",
        "\n",
        "#### Naive Bayes Pseudocode\n",
        "##### TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$)  \n",
        "$V \\leftarrow extractVocabulary(\\mathbb D)$  \n",
        "$N \\leftarrow countDocs(\\mathbb D)$    \n",
        "for $c \\in \\mathbb C$:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$N_c \\leftarrow countDocsInClass(\\mathbb D, c)$  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$prior[c] \\leftarrow \\frac{N_c}{N}$  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$text_c \\leftarrow concatenateTextOfAllDocsInClass(\\mathbb D, c)$   \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$T_{ct} \\leftarrow countTokensOfTerm(text_c,t)$  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in V$:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$condprob[t][c] \\leftarrow \\frac{T_{ct} + 1}{\\sum_{t'}(T_{ct'} + 1)}$  \n",
        "return $V,prior,condprob$\n",
        "\n",
        "##### ApplyMultinomialNB($\\mathbb C,V,prior,condprob,d$)\n",
        "$W \\leftarrow extractTokensFromDoc(V,d)$   \n",
        "for $c \\in \\mathbb C$:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;$score[c] \\leftarrow log(prior[c])$  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;for $t \\in W$:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$score[c] += log(condprob[t][c])$  \n",
        "return $argmax_{c \\in \\mathbb C} score[c]$\n",
        "\n",
        "__a) Tokenization (1pt)__  \n",
        "Implement the function `tokenize` to transform a text document to a list of tokens with the regex pattern `\\b\\w\\w+\\b`. Transform all tokens to lowercase.\n",
        "\n",
        "__b) Naive Bayes \"Training\" (6pts)__  \n",
        "Implement the `__init__` function to set up the Naive Bayes Model. Cf. TrainMultiNomialNB($\\mathbb C$,$\\mathbb D$) in the pseudocode above. Contrary to the pseudocode, the `__init__` function should not return anything, but the vocabulary, priors and conditionals should be stored in class variables. We only want to keep tokens with a frequeny >= `min_count` in the vocabulary.\n",
        "\n",
        "__c) Naive Bayes Classification (3pts)__  \n",
        "Implement the `classify` function to return the most probable class for the provided document according to your Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9d9056",
      "metadata": {
        "id": "5f9d9056"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    '''Naive Bayes for text classification.'''\n",
        "    def __init__(self, docs: List[str], labels: List[int], min_count: int=1):\n",
        "        '''\n",
        "        :param docs: list of documents from which to build the model (corpus)\n",
        "        :param labels: list of classes assigned to the list of documents (labels[i] is the class for docs[i])\n",
        "        :param min_count: minimum frequency of token in vocabulary (tokens that occur less times are discarded)\n",
        "        '''\n",
        "        # your code for Task 3b) here\n",
        "\n",
        "    def tokenize(self, doc: str):\n",
        "        '''\n",
        "        :param doc: document to tokenize\n",
        "        :return: document as a list of tokens\n",
        "        '''\n",
        "        # your code for Task 3a) here\n",
        "\n",
        "    def classify(self, doc: str):\n",
        "        '''\n",
        "        :param doc: document to classify\n",
        "        :return: most probable class\n",
        "        '''\n",
        "        # your code for Task 3c) here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bca1195",
      "metadata": {
        "id": "2bca1195"
      },
      "source": [
        "__d) Evaluation (2pts)__\n",
        "Test your implementation on the 20newsgroups dataset. If implemented correctly, with `min_count=1` your Naive Bayes classifier should obtain the same accuracy as the implementation by scikit-learn (see below for comparison)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb0288b",
      "metadata": {
        "id": "bcb0288b"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# see https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html for details\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train')\n",
        "test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(train.data)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x,train.target)\n",
        "\n",
        "pred = clf.predict(vectorizer.transform(test.data))\n",
        "\n",
        "accuracy_score(test.target,pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea57aa7",
      "metadata": {
        "id": "aea57aa7"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}