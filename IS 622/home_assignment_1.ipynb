{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmlRFdVNZeLT"
      },
      "source": [
        "Network Science FSS 2025\n",
        "\n",
        "# Home Assignment 1\n",
        "\n",
        "Submit your solution via ILIAS by **23.59 on Monday, 10th of March**.\n",
        "\n",
        "Submit your solutions in teams of 3-4 students. Unless explicitly agreed otherwise in advance, **submissions from teams with more or less members will NOT be graded**.\n",
        "Make sure that all members of your team are added to the team on ILIAS using their ILIAS account handles. As names are not unique, if the system indicates there is \"no access\", then the wrong person has been added. Submit one notebook per team, do not submit the dataset(s) you used. Also, do NOT compress/zip your submission!\n",
        "\n",
        "You may use the code from the exercises and basic functionalities that are explained in official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
        "\n",
        "#### General guidelines:\n",
        "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
        "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions.\n",
        "* Ensure that the notebook does not rely on the current notebook or system state!\n",
        "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that\n",
        "    are not in scope anymore.\n",
        "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
        "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
        "* Ensure your code/notebook terminates in reasonable time.\n",
        "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
        "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
        "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
        "* If you have any general question regarding the understanding of a task, do not hesitate to post in the student forum in ILIAS, so we can clear up such questions for all students in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYDgDeNUZeLV"
      },
      "source": [
        "Additional packages (if any):\n",
        " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yLLBvCFZeLW"
      },
      "source": [
        "### The Train Bombing Network\n",
        "\n",
        "For most of this home assignment, you will be working on the train bombing network. This is provided in an edgelist format in the file _train.edgelist_.\n",
        "\n",
        "This undirected network contains contacts between suspected terrorists involved in the train bombing of Madrid on the 11th of March 2004, as reconstructed from newspapers. A node represents a terrorist and an edge between two terrorists shows that there was contact between the two terrorists. The edge weights denote how 'strong' a connection was. This includes friendship and co-participation in training camps or previous attacks. In the following, we will denote this network as $G$.\n",
        "\n",
        "<span style=\"color:orange\">__Load edge weights, but ignore them for the calculations in Task 1 and 2__</span>\n",
        "\n",
        "__References:__  \n",
        "1) Dataset in the KONECT network repsitory: http://konect.cc/networks/moreno_train/\n",
        "2) Brian Hayes. Connecting the dots. can the tools of graph theory and social-network studies unravel the next big plot? American Scientist, 94(5):400--404, 2006."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zDPKmjjJZeLX"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from typing import List, Optional, Tuple, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rPpdxtIZeLY"
      },
      "source": [
        "### Task 1:  Basic Network Properties (8 pts)\n",
        "\n",
        "__a)__ Read in the data file and store the network as variable `G`. Store the number of nodes and edges of `G` into variables `n_nodes` and `n_edges` respectively. **(2 pts)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_gp6IvKCZeLY",
        "outputId": "ca889ff1-424b-475b-d646-bfbce45cfb5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "243\n"
          ]
        }
      ],
      "source": [
        "G = nx.read_edgelist(\"train.edgelist\", data=[('weight', int)], comments=\"%\")\n",
        "n_nodes = G.number_of_nodes() # get the number of nodes\n",
        "n_edges = G.number_of_edges()\n",
        "print(n_nodes)\n",
        "print(n_edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7m5R0Z2ZeLY"
      },
      "source": [
        "__b)__ Compute the average degree and the density of `G`. Store them as variables `avg_degree` and `density`. Is it sparse? Explain your answer! **(2 pts)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dNCQfFk3ZeLY",
        "outputId": "e233f83d-737d-49e1-d286-824cac52b653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.59375\n",
            "0.12053571428571429\n"
          ]
        }
      ],
      "source": [
        "degrees = [edge for node, edge in G.degree()]\n",
        "avg_degree = sum(degrees) / n_nodes # get the sum of edges of each node, and devided by the number of nodes\n",
        "print(avg_degree)\n",
        "\n",
        "density = nx.density(G)\n",
        "print(density)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAPIZ8S2ZeLY"
      },
      "source": [
        "__Answer:__ it's much lower than 0.5, tends to sparse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpb48YS8ZeLZ"
      },
      "source": [
        "__c)__ Determine the network's diameter and average shortest path length. Store them as variables `diameter`and `avg_pl`. **(2 pts)**\n",
        "\n",
        "Bonus (not required to get 100% of the points, but you can gain additional points beyond 100%): Recalculate diameter and average shortest path length taking edge weights into account and store them as variables `diameter_weighted` and `avg_pl_weighted`. **(2 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.is_connected(G) # ensure it's connected"
      ],
      "metadata": {
        "id": "WwsorpkLpAUJ",
        "outputId": "9a7326e1-2e9b-4b15-bb89-e79838f57bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diameter = nx.diameter(G) # calculate the diameter\n",
        "avg_pl = nx.average_shortest_path_length(G) # calculate the average shortest path length\n"
      ],
      "metadata": {
        "id": "pTacurJkpHX4",
        "outputId": "561cf099-cbf2-4536-89d3-67b56c57d91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6909722222222223"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diameter_weighted = nx.diameter(G, weight = 'weight') # consider weights into account\n",
        "avg_pl_weighted = nx.average_shortest_path_length(G, weight = 'weight')\n"
      ],
      "metadata": {
        "id": "-r76LH4np-8_",
        "outputId": "7deb3b2a-b224-4a65-91a0-da24a9f74100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7509920634920637"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7CCbzHLZeLZ"
      },
      "source": [
        "__d)__ Compute the average local and global clustering coefficient and store them as `avg_lcc` and `avg_gcc`. Does the network show signs of the small world effect? Explain your answer! **(4 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMV4SCymZeLZ"
      },
      "outputs": [],
      "source": [
        "nx.average_clustering(G)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DZn28lfZeLZ"
      },
      "source": [
        "__Answer:__ _Please provide your answer here!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4-sS9SZeLZ"
      },
      "source": [
        "### Task 2: Node Centralities (15 pts)\n",
        "\n",
        "In this task, we consider the following four node centrality measures:\n",
        "\n",
        "1. Degree Centrality (DC)\n",
        "2. Closeness Centrality (CC)\n",
        "3. Betweenness Centrality (BC)\n",
        "4. Eigenvector Centrality (EC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjzM0NTwZeLZ"
      },
      "source": [
        "__a)__ For each of the four measures, compute and store the corresponding centrality values of all nodes in the network in dictionaries `DC`, `CC`, `BC` and `EC`! The keys of the dictionaries should represent the node IDs and the corresponding values should represent the centrality of that node.\n",
        "\n",
        "Additionally, for each of the four measures store the node IDs (not the centrality values) with the 10 highest centrality values in lists `DC_top`, `CC_top`, `BC_top` and `EC_top`. They should be in descending order such that the first node in each list should have the highest centrality value and so on. **(3 pts)**\n",
        "\n",
        "**Example:** _In the example below node 4 has a betweenness centrality of 0.01, which is the third highest value in the network of 4 nodes, as denoted by its third position in list `BC_top`:_\n",
        "\n",
        "`BC = {'1': 0.05, '2': 0.221, '3': 0.0, '4': 0.01}`\n",
        "\n",
        "`BC_top = ['2', '1', '4', '3']`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5WqcsPeZeLZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpYGzrQNZeLZ"
      },
      "source": [
        "__b)__ For each of the four measures, compute the average and maximum distance of the most central node to all other nodes in the network. Store the average distances as `DC_avg`, `CC_avg`, `BC_avg`, `EC_avg` and maximum distances as `DC_max`, `CC_max`, `BC_max`, `EC_max`. **(4 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EB9xuA9iZeLZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_PKRz1ZeLZ"
      },
      "source": [
        "__c)__ For each of the four centrality measures, scale all node centralities in the graph such that their maximum is 1, i.e., divide them by the maximum value occuring in the network. Store the updated node centralities as `DC_scaled`, `CC_scaled`, `BC_scaled` and `EC_scaled`, in the same format as in 2a. For each centrality measure, plot the graph in a spring layout with node colors according to their centrality. Use the \"coolwarm\" colormap from matplotlib for this coloring. Make sure all networks have the same orientation! **(5 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVJydYO4ZeLZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdN8hdOrZeLa"
      },
      "source": [
        "__d)__ After looking at these measures simultaneously, we look into how those measures differ from each other.\n",
        "Use your node-wise similarities computed in a) to compute the correlation coefficient[<sup>1</sup>](#fn1) of all node-wise similarities between any two centrality measures. Store the correlation coefficients as variables `DC_CC`, `DC_BC`, `DC_EC`, `CC_BC`, `CC_EC` and `BC_EC` (e.g. store the correlation coefficient of the degree centrality (DC) and the closeness centrality (CC) in `DC_CC`).\n",
        "\n",
        "Which pair of centrality measures is the most/least correlated, and which measure is most/least correlated with all other centrality measures (average of pairwise correlations)? Argue why that is the case! **(3 pts)**  \n",
        "\n",
        "<span id=\"fn1\"><sup>1</sup>[Pearson product-moment correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) (available in numpy)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCmW-GCMZeLa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I6ce2pKZeLa"
      },
      "source": [
        "__Answer:__ _Please provide your answer here!_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Qc1DmaZeLa"
      },
      "source": [
        "### Task 3: Weak Ties (9 pts)\n",
        "\n",
        "Having looked at the nodes of the network, we now consider the (weighted) edges of the network.\n",
        "\n",
        "__a)__ Take a look at the distribution of edge weights. Which edge weights are present in the network, and how often does each edge weight occur? Plot these occurences using a histogram. **(2 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwLblkcNZeLa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nKpX13iZeLa"
      },
      "source": [
        "__b)__ Write a function that computes the neighborhood overlap score of a given edge, using the function signature which is specified in the cell below. Note that we want to return -1 if the edge does not exist in the network. For an edge between nodes $u$ and $v$, we do not count $u$ and $v$ in the union of neighbors in the denominator. **(4 pts)**\n",
        "\n",
        "**Example:** _Let `H = nx.from_numpy_array(np.array([[0,1,1,1,0],[1,0,0,1,1],[1,0,0,1,0],[1,1,1,0,0],[0,1,0,0,0]]))` be an undirected NetworkX graph. Your implementation of `neighborhood_overlap` should return the same output as in the given examples below. Please note that correct output values do not necessarily mean that you have implemented the function correctly. Ideally, you should come up with your own data to test your function._\n",
        "\n",
        "`neighborhood_overlap((0,1), H) == 0.3333333333333333`\n",
        "\n",
        "`neighborhood_overlap((0,2), H) == 0.5`\n",
        "\n",
        "`neighborhood_overlap((0,3), H) == 1.0`\n",
        "\n",
        "`neighborhood_overlap((1,4), H) == 0.0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFDzH6uXZeLa"
      },
      "outputs": [],
      "source": [
        "# INPUT PARAMETERS\n",
        "#\n",
        "# edge: pair of node IDs that represents the edge we want to compute the node overlap on.\n",
        "# G: networkx graph whose nodes we want to check. You can assume that G is undirected, but weighted.\n",
        "#\n",
        "# RETURN VALUE\n",
        "# the node overlap of the given edge as a float\n",
        "\n",
        "def neighborhood_overlap(edge, G):\n",
        "    # your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnk9E3mPZeLa"
      },
      "source": [
        "__c)__ Apply your neighborhood overlap function on the network $G$, and save all edges which are local bridges as tuples into a list `lb`. The first two values of the tuple should be the nodes of an edge, while the third value should represent the weight of this edge. Save only edges that are truly local bridges, i.e., through which information from nodes other than the two adjacent nodes can flow. Again, plot the graph using a spring layout with the same orientation as in task 2, with all nodes being blue, and color all edges which are local bridges in red. **(4 pts)**\n",
        "\n",
        "**Example:** _In the example below there are three local bridges in total. The first local bridge is an edge between the nodes 6 and 33 with an edge weight of 1:_\n",
        "\n",
        "`lb = [('6', '33', 1.0), ('8', '39', 1.0), ('8', '48', 1.0)]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l1GEWi2ZeLa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}